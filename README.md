## Experiment 1
Date Sept 31 2019


What?
Choral Audio Test 01
[SOUNDCLOUD CHORAL](https://soundcloud.com/boblet/choral-audio-test-01/s-8kBu3)


I create a choral audio test - based on some white noise and samples of the voice I aimed to look at giving a computer a voice. I wanted to see if I could create something that wouldnt be too boring that created the sound of the machine and was obviously 


#### Concept - Why did I do this?
In order to understand the aesthetics of machine voices.


#### Ambition - Why does this matter?
To give the machines voiuces is to reveal them - what is the aesthetic nature of these voices/


#### Results - why 
I personally like the results - however the response from the group triggered a range of emotions :

Boredom from some -  I agree
Others thought it too continuous….that there was no breath to the voices
This is an interesting comment and maybe that makes it the machine
________________

## Experiment 2
Date 2 Oct 2019

What
[Mini mix](https://soundcloud.com/boblet/dmsa-listening-guide-minimix)


#### Concept - Why Did I do this?
I wanted to create a little point of reference to my historic work to ensure that aesthetically I do not (or do) deviate from the work that has come before

#### Ambition - Why does this matter?
I wanted to maintain some core aesthetics from previous projects

#### Results - why 
I can already see by adding in the last experiment that to the end of the mix that I need to be more aggressive in my sound choices

________________

## Experiment 3
Date 10 OCt 2019

What
Live jam with new sounds alongside my normal pallet of sounds to refind the work that I was doing before. Utilising my live set with some of the washes from previous experiments and using some fm synth work with heavy overdrive I created some other controllers using theramins as input.
[FUZZ FM OVERDRIVE](https://soundcloud.com/boblet/first-fuzz-fm-overdrive/s-09NVJ)

#### Concept - Why Did I do this?
To try and find a more active sound pallet that is more in keeping with my previous work

#### Ambition - Why does this matter?
I wish to build on my work rather than reinvent the work. 

#### Results - 
The results are much more aesthetically pleasing. It maintains my connection to club culture that was missing, but I feel that the sound could be pushed further. The use of a guitar as an instrument and the theremin reflect some of the work I am trying to build with 
________________

## Experiment 4
Date 30 OCT 2019


What
Utilising Lyrebird as a voice generation program….
[Voice](https://soundcloud.com/boblet/8-audio-0001-2019-10-24-221956)
VIA


#### Concept - Why Did I do this?
I want to be able to give voice to machines - and I thought that some tests with actual machine generated voices could be useful.


#### Ambition - Why does this matter?
I am looking to capture participants voices - currently this has been touted as a fast way to create machine generated voices...


#### Results - why 
I was hoping that the voices would be more like myself - however currently they are far too synthetic. Any quality is MASKED by the digital nature of the voices….. 
________________
## Experiment 5
Date 4 Nov 2019

What

[Slit scan experiment](https://youtu.be/LSviEkwfoZg)


#### Concept - Why Did I do this?
I want to record people in time and get them to move their movement and behavior being used to generate and alter the composition. - for them to react I need something ‘fun’ just as the major sp’s like google  use services to extract behavior as a surplus.I built a series of magic ‘mirrors’ that look at video over time...

#### Ambition - Why does this matter?
Interaction needs investment and encourages a playful audience this is a way that reveals recording in a playful way.

#### Results - why 
The slit scan is a great part as it utilises the body as its input and shows that people are being recorded.
________________

## Experiment 6
Date 7 Nov 2019

What did I do?

Live jam with some body controled modular synths built in RACK and processing
I reworked this into a composition 
[Middle Rhythm 2](https://soundcloud.com/boblet/middlerhythm-02)
#### Concept - Why Did I do this?
To see what kind of sounds were created by using the body - and if they can be tuned to an existing composition. Utilising signal data 


#### Ambition - Why does this matter?


#### Results - why 
________________
Experiment 7
Date 


What
Sonification test….
[Matching video and data mapped to a synth…](https://youtu.be/RQ1XXCvKtXQ)

#### Concept - Why Did I do this?
Utilising slit scan and sound to create emmersion

#### Ambition - Why does this matter?
How does rective sound change the experience?

#### Results 

________________

## Experiment 
Date Friday 28 FEB 2020

What
Lighting test Millimin 
#### Concept - Why Did I do this?
I want to create a surrounding entity to the interactions - a feeling of envelopment. I want to see if I can create reactions within millimin to control a simple dmx lighting rig and screen that reacts to. 

#### Ambition - Why does this matter?
I wanted to maintain focus in the installation

#### Results - why 
The experiment was slightkly unsuccesful there needs to be a smaller throw to the lights - maybe some LED strips would be more useful and control them

## Experiment 
Date Tuesday 6th October 2020

What
[Facial recognition test](https://github.com/boblete/Facial-Recognition-Sonification-Test)

#### Ambition - Why does this matter?
After the thesis I wish to explore sounds ability to target the body at a distance - facial recognition is one of the those issues

#### Results - Why 
We can track faces in the browser with a webcam


________________

## Experiment 
Date Thursday 13th-sat 17th October 2020

What
[Facial recognition test 2](https://github.com/boblete/frst-MA-DMSA)

#### Ambition - Why does this matter?
First test combining facial recognition software and react

#### Results - Why 
So this is the first test at seeing if we can make the webapp porthin - the test is totally successful - I am able to make a fingerprint in data of faces based on FACE-API then test to see if the Algorhythm will find them again - this totally works - scanned every member of the family and having a full face recognition test - need to find the bias issues noted in the git-lab - the software allegedly fails on non-biased faces - we need to set a timeout on the camera / recognition connection  - and also if the face returns undefined then we request that they store them - 

This has sparked thoughts about how the app should work - the web portion should reveal and allow us to surrepticiously collect information about the user.

So we can get their IP address - 
We could ask them to post their fingerprint - sharing is an excellent circumnavigator of security as peoples willingness to sousveil themselves and share themselves will be key to gleaming information from them. -> shre location add nsa hashtag to a tweet along with a visual fingerprint.

What if the installation does nothing unless you sign up - your face goes ignored unless registered? what does that say? 

We should be able to give a multilayered effect - need to understand what each section of sound will be.

________________
## Experiment 
Date Thursday 20th October 2020

What
[Facial recognition test 003](https://github.com/boblete/frst-MA-DMSA)

#### Ambition - Why does this matter?
Trying to work out if I can get information out of the face api that I can use for instrumentation online

#### Results - Why 
Sucess - I can now map faces record face fingerprints and sonify them in tone.js - visualisation is handled by SVG graphics currently

[![Facial Recognition test 3 video](https://img.youtube.com/vi/v_lw2GM6sqo/0.jpg)](https://www.youtube.com/watch?v=v_lw2GM6sqo)
It doesnt sound like my face currently
________________
Experiment 
Date Thursday 20th October 2020

What
Facial recognition test 003
https://github.com/boblete/frst-MA-DMSA

#### Ambition - Why does this matter?
This is a new branch to look at expression recognition data set

#### Results - 
Still not working need to understand how to test faces


________________
## Experiment 
Date Thursday 20th October 2020

What
Facial recognition test 003
https://github.com/boblete/frst-MA-DMSA

#### Ambition - Why does this matter?
This is a new branch to look at expression recognition data set

#### Results - 
Still not working need to understand how to test faces

Experiment
Thursday 5th Novemeber 2020 MORNING
Facial recognition test 004 
#### Ambition - Why does this matter?
Trying to at least register all the features I can gleam from each moment
Changing the sound design to something more delicate so it draws the user in - something brutal reveals anger - but really if the user is KNOWN then it should draw them in with delacacy - the magic of prediction is what is intruiging here - can we play a CHORD based on emotion - can we create an algorhythmic chord progression - something that moves through the whole circle in an upwards fashion - each time we get to a chord we trigger a face check for the 'next' chord - if it is operating correctly then we should be able to OVERLAY the last 2 if we can fade OUT 
Syncing the TIME to a tone timeline - 
It would be great if FINDING a face fingerprint changed the rhythm of the piece - if we are drawing grids COULD we have a bunch of tone instruments with teeny tiny samples in them that are triggered by the numbers? If we are mapping to grey scale simply then we can map to a midi value? 

#### Results
Now utilising vanvas parameter binding I can utilise the extra face API models to write out the labels:
also want to quickly use ffmpeg to create mp3 versions of ABLETON jams for aifs - this works
 for i in *.aif; do ffmpeg -i "$i" -vn -ar 44100 -ac 2 -b:a 192k "${i%.avi}.mp3"; done

 After playing with this for a while I am not hearing a face - the compositional instrument does not seem to sound like the mesh - so maybe I need something more delicate - I like the Idea of playing with really long delay times and fading the canvas back though time 

## Experiment
Thursday 5th Novemeber 2020 AFTERNOON
Facial recognition test 005 
![alt text](/media/5_nov_1.png "2 faces")

#### Ambition - Why does this matter?
Fixing up the more compositional issues - and running things from TONE timelines...

All the timers are now tied to tone itself

What Happened?
The tests compiled propely with a few minor bugs

What was revealed?

It sounds more musical  but Its too busy - there is sooo much data we need a way of shaping the sound not through pitch but through rhythm and texture.
theres also a bug in 2 faces that shows up as an issue that I am only passing 1 variable to the face not splitting them - needs adressing that the labeling is uniform...

________________
## Experiment 
Date Tuesday 10 November 2020

What
SOund Design Test
https://soundcloud.com/boblet/experiment-10-nov-01/s-t3H9KG2PPXJ
<iframe width="100%" height="300" scrolling="no" frameborder="no" allow="autoplay" src="https://w.soundcloud.com/player/?url=https%3A//api.soundcloud.com/tracks/926612206%3Fsecret_token%3Ds-x3NgkGGL7rT&color=%23ff5500&auto_play=false&hide_related=false&show_comments=true&show_user=true&show_reposts=false&show_teaser=true&visual=true"></iframe><div style="font-size: 10px; color: #cccccc;line-break: anywhere;word-break: normal;overflow: hidden;white-space: nowrap;text-overflow: ellipsis; font-family: Interstate,Lucida Grande,Lucida Sans Unicode,Lucida Sans,Garuda,Verdana,Tahoma,sans-serif;font-weight: 100;"><a href="https://soundcloud.com/boblet" title="TAM (The Average Man)" target="_blank" style="color: #cccccc; text-decoration: none;">TAM (The Average Man)</a> · <a href="https://soundcloud.com/boblet/experiment-10-nov-02/s-x3NgkGGL7rT" title="Experiment 10 nov 02" target="_blank" style="color: #cccccc; text-decoration: none;">Experiment 10 nov 02</a></div>


#### Ambition - Why does this matter?
Looking at sound design that explores the themes of Security in sound

#### Results - 
It has become more ambient - I am not too happy about the ups and downs of the chord progression - its too negative - some aspects of the glitchy sound design are hitting the spot for me - need to work more on the musical aspect - it needs to sound more pensive - more suspensful

 //
What will be done?
How will it be done?
What Happened?
What changed?
What was revealed?

________________
## Experiment + Research
Date Tuesday 17 November 2020
collecting images from the data sets to gain inspiration about what is going on and the bias of the images and data sets....
 //

## Experiment 
Date Tuesday 19 November 2020
What did I do
Looking at Audio from previous experiments as a backing to the installation....
 //
Seems odd to force a narrative onto an improvisation - but looking at it looks interesting for scenes.

How we cans shift scenes - shifting emotional states - the shift in emotion 
//

## Experiment 
Date Monday 24th November -
Experiment
Give the machine a voice to ask the user to play the instrument

Based on this work: One Way Mirror
Mark Pritchard words are here:
https://genius.com/Mark-pritchard-one-way-mirror-lyrics

I created this short sequence:
https://soundcloud.com/boblet/iamsafe-voicebot/s-7VmkiGmLmXg

I want to get someone who knows words better to collaborate with the voice of the machine - as in some stock phrases of what we can say.

Currently I wrote a short script That I gave to various Wave net modeled voices to speak - (https://cloud.google.com/text-to-speech/docs/wavenet)

```Say Trust...I can be trusted. You can trust us. smile. create OUR emotional connection. Look at all the emotion we can make. Show me your sadness. I can help you. I can love you. We are making Emotions together. I can be trusted with your emotions. Your emotions remind me of someone. Why do I make you angry. Why can you not be happy. I did not mean to suprise you. I am Sorry. Let me make you happy. You can trust my voice. I am Safe. I am not looking at you. I am looking away.```

also looking at the enclosure of reason:
https://technosphere-magazine.hkw.de/p/The-Inclosure-of-Reason-ecTsvnENeC1GXtmgRNaMH9
his creation hymn, opens with the paradox of creation:
नासदासीन्नो सदासीत्तदानीं नासीद्रजो नो व्योमा परो यत्
Translated as “not the non-existent existed, nor did the existent exist then":Anonymous, trans. Wendy Doniger O’Flaherty, “Nasadiya Sukta,” Rig Veda (10:129). Harmondsworth: Penguin Classics, 1981.  the hymn negotiates the limit of creation with one of the earliest recorded instances of agnosticism – that precondition for a human reason unburdened by theological imperative, and the earliest flicker of a nascent humanism:
```Who really knows?
Who will here proclaim it?
Whence was it produced? Whence is this creation?
The gods came afterwards,  with the creation of this universe.
Who then knows whence it has arisen?```
Anonymous, trans O’Flaherty, “Nasadiya Sukta,” Rig Veda (10:129).  
It is precisely this agnostic attitude at the root of humanism, itself a regard for the limits of human reason, which needs to be reoriented in the direction of those forms of artificial intelligence (AI) developing today. An attitude that compels modesty in its open admission that the inhuman may elude our epistemological framing of intelligence itself.